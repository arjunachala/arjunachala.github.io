---
layout: post
title: Dataset
---

A DATASET is the fundamental data structure in ECL, similar to RDD in Spark. Hence, understanding the fundamentals of a DATASET will empower developers to think in terms of data manipulation as a science rather than a programming chore.

A DATASET is an immutable in-memory representation of a data file (aka Logical File) that exists on the file system or data constructed entirely in-memory as a part of an execution step in ECL. An HPCC Logical file is partitioned based on the number Thor Slave processes or Roxie worker processes. The following is an example of a 100K record logical file partitioned across 4 Thor Slave processes: 

![](/assets/images/Slide16.PNG)

Hence, a DATASET is a logical pointer of 1 to N parts of a file that is (respectively) distributed across 1 to N slave processes. Any file that is either imported manually or persisted as part of a program execution, is automatically partitioned by the system and assigned to the Slave processes. 

In an earlier [post](https://arjunachala.github.io/2017/09/27/I-Wish-I-Knew-This-About-E-C-L.html) we covered parallelism offered by optimizations within an ECL data flow. In this post, we will cover HPCC's efficiency as derived from its ability to processes distributed data in parallel. The execution of instructions that work on every partition of a single DATASET, independently and transparently, is what makes it uniquely powerful.  

The physical partitions that make up a DATASET are formed in one of the following ways:

1. During the data import process, when a file is imported into the cluster using the Spray process
1. Data is distributed programmatically using the DISTRIBUTE ECL instruction. This is equivalent to the Spark RDD.parallelize function
1. Data is exported from one HPCC cluster to another HPCC cluster
1. ECL executables are deployed to Roxie

For the sake of this discussion, we will worry about the first two cases above. In the case when the data is imported (a process called SPRAY) from a file to the DFU, the file parts are broken up into almost equal partitions, one for each Slave process. The raw input ordering of the data is preserved in this case.

Now the question of the day is - "If the data is already partitioned, why do we need to worry about redoing the partitions during the execution of a program?" The answer to this is the key to understanding distributed data processing. 

Let us consider a simple Employee DATASET example:


| Employee ID   | First Name    | Last Name     |    State      |
| ------------- |:-------------:|:-------------:|:-------------:|
|878907|JOHN|SMITH|GA|
|878909|MARY|SMITH|GA|
|878444|JACOB|POLLOCK|FL|
|878444|RAJA|RAJAN|GA|
|...|...|...|...|

100,000 such records randomly distributed across 10 slave processes would be split into approximately 10,000 records per process. If we are working on record transformations that are independent of every other record, then a random distribution will achieve the maximum parallelization. However, If we are working on record transformations that would need access to adjacent records on the same slave, then we will have to consider a different distribution strategy.

For example, if we are working on performing a transformation that would need all GA state records on the same node, then our distribution strategy has to distribute the data based on the STATE field. Now, basing a distribution strategy purely on the needs of the processing logic might also lead to undesired side effects. For example, let us say our employee DATASET has a 90% coverage from the state of GA. That is, around 90,000 employees out of the 100,000 are from the state of GA. When this DATASET is distributed by the state field across a 10 slave node HPCC cluster, a single slave partition will consist of 90,000 records. The 9 other slaves will have the other 10,000. Hence, when a workload is executed, the slave containing the 90,000 of the records is doing all the work. This is not an efficient use the HPCC system.

To conclude, distributed data processing has an element of knowing the science behind the data, before implementing the processing. For example, understanding the distribution of the fields helps in the implementation of a partition strategy that maximizes the benefits of HPCC.