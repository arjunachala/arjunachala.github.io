A DATASET is the fundamental data structure in ECL, similar to RDD in Spark. Hence, understanding the fundamentals of a DATASET, will empower developers to think in terms of data manipulation as a science rather than a programming chore.

A HPCC DATASET is immutable. That is, once a DATASET is written, it can be read, manipulated and then persisted as **new** DATASET. You cannot delete, update or insert records into an existing DATASET. Before we dig deeper into the usage of a DATASET, it might make sense to understand the workings of the HPCC distributed file unit (DFU). 

<img src="/assets/images/hpcc_fs.jpeg"/>

Very simply, a DATASET comprises of 1 to N parts of a file that is (respectively) distributed across 1 to N slave nodes. Any file that is either imported manually or persisted as part of a program execution, is automatically partitioned by DFS, distributed across to the nodes and then stored on the nodes local storage. 

In addition to the parallelism offered by optimizations within a data flow, HPCC's efficiency is derived by its ability to processes distributed data in parallel. The execution of instructions that work on every partition of a single DATASET, independently and transparently, is what makes it uniquely powerful.  

The physical partitions that make up a DATASET are formed in one of the following ways:

1. During the data import process, when a file is imported onto the cluster using DFS
1. Data is distributed programmatically using the DISTRIBUTE ECL instruction. This is equivalent to the Spark RDD.parallelize function
1. Data is exported from one HPCC cluster to another
1. ECL executables are deployed

For the sake of this discussion we will worry about the first two cases above. In the case when the data is imported (a process called SPRAY) from a file to the DFU, the file parts are broken up into almost equal partitions, one part for each processing slave node. The raw input ordering of the data is preserved in this case.

Now the question of the day is - "If the data is already partitioned, why do we need to worry about redoing the partitions during a programs execution?" The answer to this is the key to understanding distributed data processing. 

Let us consider a simple Employee DATASET example:


| Employee ID   | First Name    | Last Name     |    State      |
| ------------- |:-------------:|:-------------:|:-------------:|
|878907|JOHN|SMITH|GA|
|878909|MARY|SMITH|GA|
|878444|JACOB|POLLOCK|FL|
|878444|RAJA|RAJAN|GA|
|...|...|...|...|

A 100,000 such records randomly distributed across 10 slave nodes would be split into approximately 10,000 records per node. If we are working on record transformations that are independent of every other record, then a random distribution will achieve the maximum parallelization. However, If we are working on record transformations that would need access to adjacent records on the same slave, then we will have to consider a different distribution strategy.

For example, if we are working on performing a transformation that would need all GA state records on the same node, then our distribution strategy has to distribute the data based on the STATE field. Now, basing a distribution strategy purely on the needs of the processing logic might also lead to undesired side effects. For example, let us say our employee DATASET has a 90% coverage from the state of GA. That is, around 90,000 employees out of the 100,000 are from the state of GA. When this DATASET is distributed by the state field across a 10 slave node HPCC cluster, a single slave partition will consist of 90,000 records. The 9 other slaves will have the other 10,000. Hence, when a workload is executed, the slave containing the 90,000 of the records is doing all the work. This is not an efficient use the HPCC system.

To conclude, distributed data processing has an element of knowing the science behind the data, before implementing the processing. For example, understanding the distribution of the fields helps in the implementation of a partition strategy that maximizes the benefits of HPCC.